---
title: "Student Workshop O-PLS-DA"
author: "Lucy Grigoroff"
date: "November 2023"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Student Workshop O-PLS-DA}
  %\VignetteEngine{knitr::rmarkdown}
  \usepackage[utf8]{inputenc}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

Package installation 
#change after 1st release 

```{r}
devtools::install_github("phenological/mva-plots")
library(mva.plots)
```

Example Data

Using mtcars as if it contained analyte information as well as metadata about subjects for the purposes of this tutorial. 

```{r}
exampleData <- mtcars

colnames(exampleData) <- c("metab1", "metab2", "metab3", "metab4", "metab5", "metab6", "age", "status", "sex")

exampleData$status <- ifelse(exampleData$status == 1, "treatment", "control")
exampleData$sex <- ifelse(exampleData$sex == 1, "male", "female")
rownames(exampleData) <- paste0("subject", 1:nrow(exampleData))

exampleData
```

## Analysis Types

**Orthogonal Partial Least Squares (O-PLS)** is a supervised form of multivariate analysis. Unsupervised analysis, such as Principal Component Analysis (PCA), should take place before this. PCA is unsupervised because it does not attempt to make a model based on defined outcomes when conducting the analysis. You don't define any outcome variables for its calculation. Regression is an example of a supervised learning method because it uses a known set of outcome values (independent variables) and builds a model to connect the predictor variables (sometimes called "features" in machine learning) to these outcomes. Instead, PCA looks at the properties of the data using the variance.

### Unsupervised

A PCA allows us to visually represent high-dimensional data. It does so by transforming a large set of variables into a smaller one that still contains most of the information in the large set. 

A brief look at PCA:

The data is usually standardised using mean centering and standard deviation scaling prior to principal component calculation (you have the option to not to do one or both in the following function using *center* and *scale* arguments). There are two popular methods of calculation; eigenvalue decomposition of the covariance matrix and singular value decomposition (SVD). In the mva-plots we use the prcomp function from the stats package, which uses SVD.

Using the mva-plots package **PCA()** function we can calculate our PCA model, in this case we chose to use only the first 5 metabolites. Ensure you do not provide metadata information to the *data* argument. You may which to determine how many PCs it take to explain a certain proportion of variation. In which case provide *cutoff*, in this example 95% of the variance should be explained. If no cutoff argument is provided the default is 99%. You can instead manually set the number of components using *rank.*. *modelPCA* is now an R object that contains different slots containing information about the PCA model - this includes the scores and loadings for each of the principal components as well as other summary information.

```{r}
modelPCA <- PCA(data = exampleData[,1:5], 
                optns = list(cutoff = 95))

```

This is a combined scree plot and cumulative variance plot. They represent the same thing, how much variance is explained in each component. PC1 and PC2 usually explain the majority of variance, here there is approximately 80% explained in PC1 and 11% in PC2. If you'd like to access the information as a data frame you can do so from the modelPCA R object created.

```{r}
modelPCA[["data"]][["pcSum"]]
```

You should use explained variance to help gauge how many PCs you'd like to observe using their scores and loadings. Often, we use clustering in the scores as an initial discovery analysis about points of interest. In the current example, you may only be interested in the first two components and be interested in the persons treatment status, suspecting these metabolites may be somehow important. You can use the **plotScores()** function with *color* coding according to treatment status, along with *ellipse* based on color, to help visualize possible separation/clustering. Further input parameters include *shape*, *alpha* and *size* assignment as a basic aesthetic or related to a feature, customised continuous or discrete color palettes options (*continuousPalette* and *discretePalette*) and arguments you would supply through *theme()* when using ggplot to define the plot appearance. Check the function documentation for further details and examples.

```{r}
plotScores(model = modelPCA, 
           optns = list(PCi = 1, 
                        PCj = 2, 
                        color = exampleData[, "status"],
                        colorTitle = "Status",
                        ellipse = "color"))
```
Each point represents a subject (a row of the exampleData of metabolites 1 through 5). The percentages in the axis labels describe the amount of explained variation by the respective PC.

You may want to observe outliers using Hotelling's ellipse and identify them by their subject number. The confidence interval (*ci*) for ellipse calculations is adjustable and the default is 95%. There are other available options for ellipse calculations, check the function documentation for further details and examples.

```{r}
plotScores(model = modelPCA, 
           optns = list(PCi = 1, 
                        PCj = 2, 
                        color = exampleData[, "status"],
                        colorTitle = "Status",
                        ellipse = "hotellings",
                        outlierLabels = rownames(exampleData)))
```

If you would like to view more PCs, but aren't sure how many, don't supply an argument and the threshold to explain the *cutoff* you provided for initially building the modelPCA will be used to determine the number of PCs displayed. To manually set the number of components you want to view (three in our example), provide the *thresh* argument. 

```{r}
plotScores(
  model = modelPCA,
  optns = list(
    thresh = 3,
    color = as.factor(exampleData$status),
    colorTitle = "Status",
    ellipse = "color"
  )
)

```
Separation of subject status is only evident in the first component, and you can view the loadings graphically using **plotLoadings()**. You may do as you did with **plotScores()** and view them as a grid for multiple components by providing *thresh* and not *PCi* and *PCj*, or not providing any of them, just the model.

```{r}
plotLoadings(model = modelPCA,
             optns = list(PCi = 1, PCj = 2))
```
Alternatively, this information (and the scores) will be in the modelPCA R object we created and you can access the data frame of this information.

```{r}
modelPCA[["data"]][["loadings"]]
```

```{r}
modelPCA[["data"]][["scores"]]
```

The location of the metabolite in the loadings plot relates to the patterns seen in the scores plot. In our example for PC1, metabolite 1 drives more of the treatment separation than metabolite 5 and metabolites 2, 3 and 4 drive the control separation we see. 

## Supervised Analysis

After identifying the separation in PC1, a supervised analysis can be used to attempt explicit modelling with outcomes (subject status in this case). The mva-plots packages used **ropls** for its calculations, you can find extra information on arguments and output using *?ropls::opls*. 

In this case, if we wish to model subject status with the metabolites, we are using an O-PLS-DA, where DA is Discriminant Analysis. This is the case when there are two distinct groups as your outcome (control and treatment). If your outcome is continuous, the DA does not apply (just a PLS or O-PLS). Think of your outcome as *Y* (subject status) and the features (metabolites) you want to model against it as *X*. The **oplsda()** function is used in our example to create the model. Use the *type* argument to select either a PLS or OPLS model. Where PLS only has a predictive component, OPLS has orthogonal and predictive. 

Note that you can modify how the model is built. 

If you would like more than one particular component, you can specify this. For an OPLS you may supply *orthoI* for the amount of orthogonal components and for PLS you can supply *predI* for the number of predictive components. Manually choosing the number of orthogonal components for O-PLS must be done with caution and based on the amount of data variation that is related to the outcome variable Y. Not enough components gives an under-fitted model (it does not accurately represent the data). Too many orthogonal components gives an over-fitted model (it will describe patterns specific to only the current data set). Cross validation is used to automatically select the amount of components. If you choose your own, there are metrics to judge the fit of the model which we will mention here. 

The model is cross-validated when being built using k-fold cross validation. The data is split into k subsets used for training and testing. Each are used iteratively as a test set where the other k-1 subsets which are used to train for the model (fitting the model to emphasize separation between classes for a DA). So, the model is fit with k-1 subsets, then testing with the remaining subset, for a DA to predict the classes of that remaining subset. Predictive performance is assessed. This is repeated so all k folds are, at one point, used as a test set. You may set the number of folds, k, by supplying *crossvalI*. 

```{r}
oplsdaModel <- oplsda(Y = (exampleData[, "status"]), 
                      X = exampleData[,1:5], 
                      type = "OPLS")

```

### PLS vs OPLS

PLS and OPLS both divide variability in a data set into systematic (structured) and residual (noise), OPLS further splits the systematic variability into Predictive (that correlated with Y) and orthogonal (everything not correlated with the response Y). Another way of saying the same thing: it is attempting to separate the variability in the X data into 2 parts, Predictive of Y and Unrelated (orthogonal) to Y.

If there is only one Y variable, PLS and OPLS models fitted to the same data have identical predictive power (as long as they have the same total number of components). The advantage of OPLS is a simplified model interpretation due to its separation of orthogonal and predictive. Try to remember one is not necessarily "better" than the other.

### Model Assessment

If you choose to use O-PLS-DA, you should keep in mind the properties of your data and your PCA results. For example, if you wish to build a DA model, you will need to have a fairly even number of samples in each group (groups being treatment and control in our example). Also, if there was no separation visible in the PCA, and you build an O-PLS-DA and there is now separation evident, chances are the model has been over fitted. Having a very wide matrix (many more features than you have samples) also increases chance of overfitting. There are other metrics to help identify this in addition to your initial PCA. 

R2Y (R-squared for Y): R2Y is the percent of the variation in the results is explained by the model. It ranges from 0 to 1, where 0 indicates that the model does not explain any variance, and 1 indicates that the model explains all the variance. In the context of OPLS-DA, R2Y is a measure of how well the model captures the variation in the class memberships or groups.

R2X (R-squared for X): This metric represents the proportion of variance in the predictor variables (X) that is explained by the model. It indicates how well the model captures the variation in the predictor variables. Similar to R2Y, it ranges from 0 to 1. 

cumulative Q2(Q-squared): Used to assess the predictive performance of the model. It is calculated using cross-validation and represents the proportion of the response variable variance that can be predicted by the model (predictive variance of fit). For a continuous Y, A positive Q2 indicates good predictive performance, while a negative Q2 suggests that the model is worse than a random model (Q2 of 1 (perfect prediction) to zero or negative values (inaccurate prediction)). For a DA it ranges from 0 to 1 with 1 being perfect prediction, 0.5 being no predictive capability (it's like flipping a coin) and 0 is exact inverted prediction. 

In our example you can access this information from the *oplsdaModel* object, and do so in a way that shows you the breakdown for each component:

```{r}
oplsdaModel@modelDF
```

We can now see the breakdown of orthogonal and predictive components model metrics. To access only the summary metrics (only the cumulative numbers):

```{r}
oplsdaModel@summaryDF

```
You'll notice some additional information in the summaryDF from the model. 

pre: the number of predictive components.
ort: the number of orthogonal components.
RMSEE: the square root of the mean error between the actual and the predicted responses during cross-validation.

The following are assessments related to permutation tests on the model. Permutation is a method to see if the models performance is not by chance and it has better performance than a model based on random data. In the case of DA, randomly permuted or shuffled class labels or response variable, while keeping the predictor variables unchanged, are used to create a model. The permuted models have their Q2 and R2Y compared to model built with the real class labels. The p-values are the proportion of Q2perm and R2Yperm that are above the real Q2 and R2Y.

Note that the default number of permutations is 20 as a compromise between quality control and computation speed; it can be increased with the *permI* parameter.

pQ2: assess the significance of the model. How well the model can predict the response variable for new or unseen samples. 

$$
\frac{1 + \text{Number of permuted } Q^2_{\text{cum}} \geq \text{the real } Q^2_{\text{cum}}}{\text{number of permutations}}
$$

pR2Y: reflects the predictability of the model.

$$
\frac{1 + \text{Number of permuted } R^2Y_{\text{cum}} \geq \text{the real } R^2Y_{\text{cum}}}{\text{number of permutations}}
$$
pQ2 and pR2Y are interpreted the same way other p-values are. For example, if you had 100 permutation and none of your permuted Q2 were greater than your real Q2, you would have:

$$
\frac{1 + 0}{100} = 0.01 \ pQ^2
$$

You can view the permutation assessment graphically using **permutationPlot()**. You can change the aesthetics using *colorQ*, *colorR*, *alpha*, *size* and *shape*. 
```{r}
permutationPlot(model = oplsdaModel, 
                optns = list(colorQ = "red",
                             colorR = "blue"
                             ))
```

Here we see the 20 permuted Q2 and R2Y (as the translucent 20 blue and 20 red dots). The real Q2 and R2Y (represented by the opaque circles) are above all the permuted values. 

You can view the scores and loadings plots the same way you did for a PCA.

```{r}
ps <- plotScores(model = oplsdaModel, 
                 optns = list(
                            color = exampleData[,"status"],
                            ellipse = "hotellings",
                            outlierLabels = rownames(exampleData)
                            )
               )

```
```{r}
lp <- plotLoadings(model = oplsdaModel)

```


##Prediction 

You can use your model to make predictions on on other data (for example a different cohort). Here we generate data as if we have a different group of subject with slightly different results for the metabolites we used to build the model. 

```{r}

# Select the first 5 columns
columns_to_jitter <- c("metab1", "metab2", "metab3", "metab4", "metab5")
exampleData2 <- exampleData
# Apply jitter to the selected columns
exampleData2[, columns_to_jitter] <- lapply(exampleData[, columns_to_jitter], function(x) jitter(x, factor = 0.1))
```

To make a prediction, use the **oplsdaPredict()** function. It requires your *model* and the *newdata*, be aware you should only prodive the same information you used to build the model. In this case the first 5 metabolites. 

```{r}
predictModel <- oplsdaPredict(model = oplsdaModel, 
                              newdata = exampleData2[,1:5])

```
To observe the predicted subject status, access the *predictModel* object created.
```{r}
predictModel$predY
```
The scores (for the 1st orthogonal and predictive components for our example) are also available in the predictModel object. You can project this onto the scores for your original model. 

```{r}
dfo <- as.data.frame(cbind(oplsdaModel@orthoScoreMN, 
                           oplsdaModel@scoreMN, 
                           oplsdaModel@suppLs[["y"]]))
dfp <- as.data.frame(cbind(predictModel$orthoScoreMN, 
                           predictModel$predScoreMN, 
                           predictModel$predY))

colnames(dfp) <- c("o1", "p1", "status")
colnames(dfo) <- c("o1", "p1", "status")

dfp[, 1:2] <- lapply(dfp[, 1:2], as.numeric)
dfo[, 1:2] <- lapply(dfo[, 1:2], as.numeric)

dfp$type <- "predicted"
dfo$type <- "original"

df<- rbind(dfo, dfp)

ggplot(data = df, 
       aes(x = p1, 
           y = o1, 
           color = status, 
           shape = type)) +
  scale_colour_manual(values = c("control" = "purple", 
                                 "treatment" = "orange")) +
  geom_point(size = 3) +
  theme_bw() +
  ggtitle("Score Plot Projection")

```
The "new cohort" from exampleData2 is unsurprisingly similar to pattern from the "original cohort" from exampleData. 


\ExtraResources{

https://medium.com/analytics-vidhya/understanding-principle-component-analysis-pca-step-by-step-e7a4bb4031d9

https://learnche.org/pid/latent-variable-modelling/index  

Trygg J. and Wold, S. (2002) Orthogonal projections to latent structures (O-PLS). \emph{Journal of Chemometrics}, 16.3, 119-128.

Geladi, P and Kowalski, B.R. (1986), Partial least squares and regression: a tutorial. \emph{Analytica Chimica Acta}, 185, 1-17.
}
\



